{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chpater 8 - Dimensionality Reduction\n",
    "Curse of Dimensionality: The large number of features not only makes training extremely slow, it can also make it much harder to find a good solution. <br>\n",
    "Dimensionality redction not only speeds up training, but also is extremely useful for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "High-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. => a new instance will likely be far away from any training instacnes, making predictions much less reliable than in lower dimensions. => **the more dimensions the training set has, the greater the risk of overfitting it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction\n",
    "\n",
    "### Projection\n",
    "In most real-world problems, many features are almost constance, while others are highly correlated. <br>\n",
    "All training instances actually lie within a much lower-dimensional subspace of the high-dimensional space. <br>\n",
    "\n",
    "### Manifold Learning\n",
    "A *d*-dimensional manifold is a part of an *n*-dimensional (where *d*<*n*) that locally resembles a *d*-dimensional hyperplane.\n",
    "\n",
    "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold.\n",
    "\n",
    "The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplace that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "### Preserving the Variance\n",
    "Select a lower-dimensional hyperplane that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. <br>\n",
    "Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection one that hyperplane.\n",
    "\n",
    "### Principal Components\n",
    "PCA needs axes as many as the number of dimensions in the dataset. <br>\n",
    "The unit vector that defines the $i^{th}$ axis is called the $i^{th}$ principal component (PC).\n",
    "\n",
    "Singular Value Decompostion (SVD): decomposes the training set matrix X into\n",
    " the matrix multiplication of three matrices $U \\Sigma V^{T}$, where V contains all the principal components that we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Down to *d* Dimensions\n",
    "Projecting the training set down to d dimensions:\n",
    "$$X_{d-proj} = XW_{d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn\n",
    "It automatically takes care of centering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2) # n_components: the number of dimensions\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "Explained variance ratio: It is available vai the `explained_variance_ratio_` variable. It indicates the proportion of the dataset's variance that lies along the axis of each principal componenet.\n",
    "\n",
    "### Choosing the Right Number of Dimensions\n",
    "It is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1 \n",
    "# set n_components = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA fo Compression\n",
    "It is also possible to decompress thereduced dataset back to the original number of dimensions by applying the inverse transformation of the PCA projection. <br>\n",
    "The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the *reconstruction error*.\n",
    "\n",
    "PCA inverse transformation, back to the original number of dimensions\n",
    "$$X_{recovered} = X_{d-proj}W_{d}^{T}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_componets = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "Scikit-Learn uses a stochastic algorithm called *Randomized PCA* that quickly finds an approximation ofthe first *d* principal components. Its computational complexity is $O(m \\times d^{2}) + O(d^{3})$, instead of $O(m \\times n^{2}) + O(n^{3})$ for the full SVD approach. <br>\n",
    "Set the `svd_solver` hyperparameter to \"`randomized`\". <br>\n",
    "By default, `svd_solver` is actually set to \"`auto`\": Scikit-Learn automatically uses the randomized PCA algorithm if *m* or *n* is greater than 500 and *d* is less than 80% of *m* or *n*, or else it uses the full SVD approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "*Incremental PCA* (IPCA) algorithms allow you to split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch) # you must call the partial_fit() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy's memmap class allows you to manipulate a large array stored in a\n",
    "# binary file on disk as if it were entirely in memroy; the class loads only\n",
    "# the data it needs in memory, when it needs it.\n",
    "\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m,n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "Kernel PCA (kPCA) makes it possible to perform complex nonlinear projections for dimensionality reduction. It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters\n",
    "There is no obvious performance measure to help you select the best kernel and hyperparameter values because kPCA is an unsupervised learning algorithm. <br>\n",
    "You can simply use grid search to select the kernel and hyperparameters that lead to the best performance on a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kerneal\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(gird_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to select the kernel and hyperparameters that yield the lowest reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing reconstruction\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transformation=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the reconstruction pre-image error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE\n",
    "*Locally Linear Embedding* (LLE) is a very powerful nonlinear dimensionality reduction (NLDR) technique. <br>\n",
    "LLE works by first measuring how each training instance linearly relates to its closest neighbors, and then looking for a low-dimensional representation of the training set where these local relationships are best preserved. <br>\n",
    "This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational complexity of Scikit-Learn's LLE implementation:\n",
    "- $O(m log(m)n log(k))$ for find the *k* nearest neighbors\n",
    "- $O(mnk^{3})$ for optimizing the weights\n",
    "- $O(dm^{2})$ for constructing the low-dimensional representations => $m^{2}$ makes this algorithm scale poorly to very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques\n",
    "- *Multidimensional Scaling* (MDS) reduces dimensionality while trying to preserve the distances between the instances.\n",
    "- *Isomap* creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while try to preserve the geodesic distances between the instances.\n",
    "- *t-Distributed Stochastic Neighbor Embedding* (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart.\n",
    "- *Linear Discriminant Analysis* (LDA): the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "774950339c587ca9f62d5b99430caf26f939018233a2ad762310c7ae7196a066"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('hands_on_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
