{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Unsupervised Learning Techniques\n",
    "The vast majority of the available data is actually unlabeled. <br>\n",
    "Clustering: the goal is to group similar instances together into clusters. <br>\n",
    "Anomaly detection: the objective is to learn what \"normal\" data looks like, and use this to detect abnormal instances. <br>\n",
    "Density estimation: this is the task of estimating the probability density function of the random process that generated the dataset.\n",
    "\n",
    "## Clustering\n",
    "Cluster: a group of similar instances.\n",
    "\n",
    "Aplications of clustering:\n",
    "- Customer segmentation\n",
    "- Data analysis\n",
    "- Dimensionality reduction technique: each instance's feature vector x can be replaced with the vector of its affinities (*affinity* is any meausre of how well an instance fits into a cluster).\n",
    "- Anomaly detection\n",
    "- Semi-supervised learning\n",
    "- Search engines\n",
    "- Segmenting an image\n",
    "\n",
    "### K-Means\n",
    "The K-Means algorithm is a simple algorithm capable of clustering blobs of instances very quickly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of clustering, an instances's label is the index of the cluster that this instance gets assigned to by the algorithm. <br>\n",
    "The KMeans instance preserves a copy of the labels of the instances it was tained on available via the `labels_` instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n",
    "# array([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n",
    "\n",
    "y_pred is kmeans.labels_\n",
    "# True\n",
    "\n",
    "kmeans.cluster_centers_\n",
    "# array([[-2.80389616, 1.80117999],\n",
    "#        [ 0.20876306, 2.25551336],\n",
    "#        [-2.79290307, 2.79641063],\n",
    "#        [-1.46679593, 2.28585348],\n",
    "#        [-2.80037642, 1.30082566]])\n",
    "\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)\n",
    "# array([1, 1, 2, 2], dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Means algorithm does not behave very well when blobs have very different diamters since it cares the distance to the centroid the most.\n",
    "\n",
    "Hard clustering: assigning each instance to a single cluster. <br>\n",
    "Soft clustering: giving instance a score per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new) #measuring the distance from each instance to every centroid\n",
    "# array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n",
    "#        [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n",
    "#        [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n",
    "#        [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The K-Means Algorithm\n",
    "After locating the centroids randomly, the algorithm repeats labeling the instances and updating the centroids until the centroids stop moving.\n",
    "\n",
    "#### Centroid Initialization Methods\n",
    "If you know approximately where the centroids shoud be, set the `init` hyperparameter to a NumPy array containing the list of centroids, and set `n_init` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can run the algorithm multiple times (this is controlled by `n_init`) with different random initializations and keep the best soluthion with the lowest inertia. <br>\n",
    "*Inertia*: the mean squared distance between each instance and its closest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_\n",
    "# 211.59853725816856\n",
    "\n",
    "kmeans.score(X)\n",
    "# -211.59853725816856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*K-Means*+\\\\+: a smater initialization step selects centroids that are distant from one another, and this makes the K-Means algorithm much less likely to converge to a sub-optimal solution. <br>\n",
    "The `KMeans` class uses this initialization method by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerated K-Means and Mini-batch K-Means\n",
    "Accelerated K-Means: it accelerates the algorithm by avoiding many unnecessary distance calculations which is achieved by exploiting the trangle inequalities and by keeping track of lower and upper bounds for distances between instances and centroids. It is default by the `KMeans` class. \n",
    "\n",
    "Mini-batch K-Means: the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration instead of using the full dataset at each iteration. <br>\n",
    "Although the Mini-batch K-Means algorithm is much faster than the regular K-Means algorithm, its inertia is generally slightly worse, especially as the number of clusters increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=5)\n",
    "minibatch_kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Optimal Number of Clusters\n",
    "*Silhouette score*: the mean silhouette coefficient over all the instances. <br>\n",
    "*Silhouette coefficient*: $(b - a) / max(a, b)$ <br>\n",
    "$a$: the mean distance to the other instances in the same cluster. <br>\n",
    "$b$: the mean distance to the instances of the next closest cluster. \n",
    "\n",
    "The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster.\n",
    "\n",
    "*Silhouette diagram* & *Silouhette analysis*\n",
    "When most of the instances in a cluster have alower coefficient than the silhouette score, then the cluster is rather bad since this means its instances are much too close to other clusters. <br>\n",
    "Choose $k$ where all clusters have similiar sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, kmeans.labels_)\n",
    "# 0.655517642572828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits of K-Means\n",
    "- It is necessary to run the algorithm several times to avoid sub-optimal solutions.\n",
    "- You need to specify the number of clusters.\n",
    "- K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes.\n",
    "\n",
    "It is important to scale the input features before you run K-Means, or else the clusters may be very stretched, and K-Means will perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using clustering for image segmentation\n",
    "*Image Segmentation*: the task of partitioning an image into multiple segments. <br>\n",
    "In *semantic segmentation*, all pixels that are part of the same object type get assigned to the same segment. <br>\n",
    "In *instance segmentation*, all pixels that are part of the same individual object are assigned to the same segment. <br>\n",
    "*Color segmentation*: simply assign pixels to the same segment if they have a similar color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "image = imread(os.path,join(\"images\", \"clustering\", \"ladybug.png\"))\n",
    "image.shape\n",
    "# (533, 800, 3)    # the height, the width, and the number of color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image.reshape(-1, 3) # It reshapes the array to get a long list of RGB colors\n",
    "kmeans = KMeans(n_cluster=8).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Clustering for Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits) \n",
    "\n",
    "pipeline = Pipeline([Pipeline\n",
    "    (\"kmeans\", Kmeans(n_clusters=50)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "# 0.9822222222222222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, we created a pipeline that first clustered the training set into 50 clusters and replaced the images with their distances to these 50 clusters, then apply a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(kmeans__n_clusters=range(2, 100))\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "\n",
    "grid_clf.best_params_\n",
    "# {'kmeans__n_clusters: 90}\n",
    "grid_clf.score(X_test, y_test)\n",
    "# 0.9844444444444445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `GridSearchCV` to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Clustering for Semi-Supervised Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]\n",
    "\n",
    "y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "log_reg.score(X_test, y_test)\n",
    "# 0.9244444444444444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we clustered the training set into 50 clusters, then for each cluster we find the representative images (images closest to the centroid. Next, we manually label each image). \n",
    "\n",
    "*Label propagation*: propagating the labels to all the other instances in the same cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train_propagated)\n",
    "log_reg.score(X_test, y_test)\n",
    "# 0.9288888888888889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagating the labels to the 20% of the instances that are closest to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 20\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\\\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "log_reg.score(X_test, y_test)\n",
    "# 0.9422222222222222\n",
    "\n",
    "np.mean(y_train_partially_propagated == y_train[partially_propagated])\n",
    "# 0.9896907216494846"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Active learning*: a human expert interacts with the learning algorithm, prividing labels when the algorithm needs them. One of the most common ones is called uncertainty sampling.<br>\n",
    "\n",
    "*Uncertainty sampling*:\n",
    "- The model is trained on the labeld instances gathered so far, and this model is used to make predictions on all the unlabeled instances.\n",
    "- The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) must be labeled by the expert.\n",
    "- Then you just iterate this process again and again, until the performance imporvement stop being worth the labeling effort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "- For each instance, the algorithm counts how many instances are located within a small distance $\\varepsilon$ from it. THis region is called the instance's $\\varepsilon$-neighborhood.\n",
    "- If an instance has at least `min_samples` instances in its $varepsilon$-neighborhood (including itself), then it is considered a *core instance*. (A dense region)\n",
    "- All instances in the neighborhood of a core instance belong to the same cluster. This may include other core instances, therefore a long sequence of neighboring core instances forms a single cluster.\n",
    "- Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.05)\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "dbscan.labels_ # Anomaly if -1.\n",
    "# array([ 0, 2, -1, -1, 1, 0, 0, 0, ..., 3, 2, 3, 3, 4, 2, 6, 3])\n",
    "\n",
    "len(dbscan.core_sample_indices_)\n",
    "# 808\n",
    "\n",
    "dbscan.core_sample_indices_\n",
    "# array([ 0, 4, 5, 6, 7, 8, 10, 11, ..., 992, 993, 995, 997, 998, 999])\n",
    "\n",
    "dbscan.components_\n",
    "# array([[-0.02137124, 0.40618608],\n",
    "#        [-0.84192557, 0.53058695],\n",
    "#        ...\n",
    "#        [-0.94355873, 0.3278936 ],\n",
    "#        [ 0.79419406, 0.60777171]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DBSCAN class does not have a `predict()` method, and it cannot predict which cluster a new instance belongs to. <br>\n",
    "The rationale for this decision is that several classification algorithms could make sense here, and it is easy enough to train one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
    "\n",
    "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
    "\n",
    "knn.predict(X_new)\n",
    "# array([1, 0, 1, 0])\n",
    "\n",
    "knn.predict_proba(X_new)\n",
    "# array([[0.18, 0.82],\n",
    "#        [1. , 0. ],\n",
    "#        [0.12, 0.88],\n",
    "#        [1. , 0. ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is a very simple yet powerful algorithm, capable of identifying any number of clusters, of any shape, it is robust to outliers, and it has just two hyperparameters. <br>\n",
    "However, if the density varies significantly across the clusters, it can be impossible for it to caputre all the clusters properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Clustering Algorithms\n",
    "- Agglomerative clustering: a hierarchy of clusters is built from the bottom up.\n",
    "- Birch: this algorithm was designed specifically for very large datasets, and it can be faster than batch K-Means, with similar results, as long as the number of features is not too large (<20).\n",
    "- Mean-shift: this algorithm starts by placing a circle centered on each instance, then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shift step until all the circles stop moving. All the instances whose circles have settled in the same palce are assigned to the same cluster.\n",
    "- Affinity propagation: this algorithm uses a voting system, where instances vote for similar instances to be their representatives, and once the algorithm converges, each representative and its voter form a cluster.\n",
    "- Spectral clustering: this algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it, then it uses another clustering algorithm in this low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures\n",
    "A *Gaussian mixture model* (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. <br>\n",
    "All the instances generated from a sigle Gaussian distribution form a cluster that typically looks like an ellipsoid. <br>\n",
    "*Expectation Maximization* (EM) algorithm: it initializes the cluster parameters randomly, then it repeats assigning instances to clusters (the expectation step) and updating the clusters (the maximization step). <br>\n",
    "EM uses soft cluster assignment: for each instance during the expectation step, the algorithm estimates the probabilty that it belongs to each cluster (based on the current cluster parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection using Gaussian Mixtures\n",
    "*Anomaly dection* (also called *outlier detection*) is the task of detecting instances that deviate strongl from the norm. <br>\n",
    "Using a Gaussian mixture model for anomaly detecting: any instance located in a low-density region can be considered an anomaly. Density threshold must be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Novelty detection* differs from anomaly detection in that the algorithm is assumed to be trained on a \"clean\" dataset, uncontaminated by outliers, whereas anomaly detection does not make this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Number of Clusters\n",
    "Find the model that minimizes a *theoretical information criterion* such as the *Bayesian information criterion* (BIC) or the *Akaike information criterion* (AIC). \n",
    "\n",
    "$BIC = log(m)p - 2 long(\\hat{L})$ <br>\n",
    "$AIC = 2p - 2 log(\\hat{L})$ <br>\n",
    "- $m$ is the number of instances\n",
    "- $p$ is the number of parameters learned by the model.\n",
    "- $\\hat{L}$ is the maximized value of the likelihood function of the model.\n",
    "\n",
    "Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters), and reward models that fit the data well.\n",
    "\n",
    "Given a statistical model with some parameters $\\theta$, the word \"probability\" is used to describe how plausible a future outcome x is (knowing the parameter values $\\theta$), while the word \"likelihood\" is used to describe how plausible a particular set of parameter values $\\theta$ are, after the outcome x is known. <br>\n",
    "Given a dataset X, a common task is to try to estimate the most likely values for the model parameters. To do this, you must find the values that maximize the likelihood function, given X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.bic(X)\n",
    "# 8189.74345832983\n",
    "gm.aic(X)\n",
    "# 8102.518178214792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Gaussian Mixture Models\n",
    "The `BayesianGaussianMixture` class is capable of giving weights equal (or close) to zero to unnecessary clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n",
    "bmg.fit(X)\n",
    "np.round(bgm.weights_, 2)\n",
    "# array([0.4 , 0.21, 0.4 , 0. , 0. , 0. , 0. , 0. , 0. , 0. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture models work great on clusters with ellipsoidal shapes, they may perform poorly with different shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Anomaly Detection and Novelty Detection Algorithms\n",
    "- *Fast-MCD* (minimum covariance determinant): this algorithm is useful for outlier detection, in particular to cleanup a dataset. It assumes that the normal instances are generated from a single Gaussian distribution, but it also assumes that the dataset is contaminated with outliers that were not generated from this Gaussian distribution.\n",
    "-  *Isolation forest*: this is an efficient algorithm for outlier detection, especially in high-dimensional datasets. The algorithm builds a Random Forest in which each Decision Tree is grown randomly.\n",
    "- *Local outlier factor* (LOF): this algorithm is good for outlier detection. It compares the density of instances around a given instance to the density around its neighbors.\n",
    "- *One-class SVM*: this algorithm is better suited for novelty detection."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "774950339c587ca9f62d5b99430caf26f939018233a2ad762310c7ae7196a066"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('hands_on_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
