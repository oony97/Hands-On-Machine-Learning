{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Support Vector Machines\n",
    "## Linear SVM Classification\n",
    "SVM classifier: fitting the widest possible street between the classes. <br>\n",
    "A decision boundary is fully determined by support vectors (instances located on the edge of the street). <br>\n",
    "SVMs are sensitive to the features scales.\n",
    "### Soft Margin Classification\n",
    "Hard margin classification: strictly imposing that all instances be off the street and on the right side <br>\n",
    "Soft margin classification: find a good balance between keeping the steet as large as possible and limiting the margin violations. <br>\n",
    "Margin violation: An instance that end up in the middle of the street or even on the wrong side. <br>\n",
    "`C` hyperparameter: a smaller `C` value leads to a wider street but more margin violations, while a high `C` value makes fewer margin violation but ends up with a smaller margin. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "Adding polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "The kernel trick makes it possible to get the same result as if you added many polynomial features, even with very high degree polynomials, without actually having to add them. <br>\n",
    "`ceof` controls how much the model is influenced by high-degree polynomials versus low-degree polynomials.\n",
    "### Adding Similarity Features\n",
    "### Gaussian RBF Kernel\n",
    "Increasing `gamma` makes the bell-shape curve narrower. <br>\n",
    "=> Each instance's range of influece is smaller. <br>\n",
    "=> The dicision boundary ends up being more irregular, wiggling around indiidual instances. <br>\n",
    "=> $\\gamma$ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it, and vice versa.\n",
    "### Computaional Complexity\n",
    "Training time complexity of the `LinearSVC` class: $O(m \\times n)$ <br>\n",
    "Training time complexity of the `SVC` class: $O(m^{2} \\times n)$ or $O(m^{3} \\times n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regerssion\n",
    "SVM Regression tries to fit as many instances as possible on the street while liming margin violations. <br>\n",
    "The width of the street is controlled by a hyperparameter $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "### Decision Function and Predictions\n",
    "The linear SVM classifier model predicts the class of a new instance x y simply computing the decision function: $w^{T}x + b = w_{1}x_{1} + \\cdots + w_{n}x_{n} + b$ <br>\n",
    "$$\\hat y = \\begin{cases} 0 \\text{ if $w^{T}x + b < 0,$} \\\\ 1 \\text{ if $w^{T}x + b \\geq 0$} \\end{cases}$$\n",
    "Training a linear SVM classifier means find the value of w and b that make this margin as wide as possible while avoidng margin violations (hard margin) or limiting them (soft margin).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective\n",
    "The slope of the decision function is equal to the norm of the weight vector, $||w||$. <br>\n",
    "If $t^{(i)} = -1$ when $y^{(i)}=0$ and $t^{(i)}=1$ when $y^{(i)}=1$, then $t^{(i)}(w^{T}x^{(i)}+b) \\geq 1$ for all instances. <br>\n",
    "Hard margin linear SVM classifier objective: <br>\n",
    "minimize $\\frac{1}{2}w^{T}w$ ($=\\frac{1}{2}||w||^{2}$) (because $||w||$ is not differential at $w = 0$) <br>\n",
    "subject to $t^{(i)}(w^{T}x^{(i)}+b) \\geq 1$ for $i = 1, 2, \\cdots, m$ <br>\n",
    "Slack variable $\\zeta^{(i)}$: $\\zeta^{(i)}$ measures how much the $i^{th}$ instance is allowed to violate the margin. <br>\n",
    "The C hyperparameter allows us to define the trade-off between the large margin and the less margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Programming\n",
    "A Quadratic Programming (QP) problem is a convex quadratic optimization problem with linear constraints.\n",
    "### The Dual Problem\n",
    "If the primal problem is given, its dual problem can be expressed. <br>\n",
    "Under some conditions, the primal problem and the dual problem have the same solution. <br>\n",
    "Computing $\\hat{\\alpha}$ for the dual problem $\\Rightarrow$ computing $\\hat{w}, \\hat{b}$ for the primal problem. <br>\n",
    "The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features. <br>\n",
    "The dual problem makes the kernel trick possible, while the primal does not.\n",
    "### Kernelized SVM\n",
    "In Machine Learning, a *kernel* is a function capable of computing the dot product $\\phi (a)^{T} \\phi (b)$ based only on the original vectors **a** and **b**. $K(a, b)$\n",
    "### Online SVMs\n",
    "Hinge Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1056b5f930c3fc8e2c41bb2104f26934e8a566904ab87f07ef8b77e3e2aa0727"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
